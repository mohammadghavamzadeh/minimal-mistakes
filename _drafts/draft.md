> This post is written to gather a better understanding of recent work done in eigen option discovery using successor representation. I try to list out most of the major ideas building upto eigen option discovery and show results obtained on simple gridworld tasks.

PROTO VALUE FUNCTIONS :

These were introduced in Mahadevan et al. and can be considered as task independent basis functions. A linear combination of these can thus be used to define the value function of a particular task. They are constructed by diagonalizing a diffusion flow matrix which in turn is the Laplacian of the a function formed using the adjacency matrix. The advantage of learning proto value functions is that they represent the general topology of the state space irrespective of or in absence of the reward function. Although a large state space may have high dimensional value function vector, it can be approximately represented as a linear combination of a substantially small number of proto value functions.
Proto value functions try to move on from the idea of representing the value function in terms of a vector space and try to represent them using the manifold itself. For ex. let the optimal value function be represented by a circle, which is a one dimensional manifold. Now, estimating this value function will require us to calculate distance between any two points on the circle, which, in the ambient space ie. the two dimensional euclidean space is given by the length of the line segment joining these points. However, in reality the distance between the two points is the length of the arc formed between the two ie. distance on the manifold and not the ambient space. Distance here basically tells us how far two points or states are from each other.
The inherent smoothness in value functions is due to the bellman equations which dictate that the value at a given state is a linear function of the value at neighboring states.
In the study of continuous manifolds, the eigenfunctions of the Laplacian form a discrete orthonormal basis [Rosenberg, 1997]
The manifold can be learnt by constructing a laplacian operator matrix on the graph representing the state space, and finding its k  eigenvectors. These k eigenvectors can be used as the columns to construct the basis function matrix. Thereafter, a linear least square approximation using the optimal value function can be used to construct the value function.

EIGEN PURPOSES:  

They are described as intrinsic reward functions represented by the difference in one of the protovalue functions at two given states. Since eigenpurposes are defined using protovalue functions, they too are independent of the reward function. Such a reward will introduce a policy, which is referred as an eigenbehaviour and can be looked at as an option. Different such options are produced for different intrinsic reward functions or eigenpurposes which correspond to different protovalue basis functions. 
In the tabular case, this intrinsic reward is just the difference in proto value function at two states ie. the current and the successor state. Therefore, since each such intrinsic reward corresponds to an eigen option, the number of eigen options discoverable through this method is the number of protovalue functions which is essentially the number of eigenvectors of the combinatorial laplacian as descibed by the PVF theory. The one assumption considered in the initial idea is that the adjacency matrix of the state graph is available so as to generate the graph laplacian. The authors note that learning options which do not use the external reward and are thus task independent allow them to not only focus on finding bottleneck states, but finding general options which can help in learning the optimal policies for a varied number of tasks. They also show that learning options having goals as bottleneck states can actually hinder exploration as they only allow traversing only between the subgoals, and not other parts of the state space. Learning options that are task independent allows for more efficient learning of higher level policies.
This idea is then extended to the case when the adjacency matrix is not readily available by sampling transitions and creating an 'incidence' matrix with each row being the difference in the feature representation of the sampled transition. SVD is performed on this matrix to get the eigenpurposes as the columns of the V matrix. Avoiding appending a sampled transition again in such an 'incidence' matrix allows the authors to extend this to the continuous state case.
The next work in this line tries to establish a link between finding such eigenoptions using the successor representation instead of using protovalue functions. The authors show that the eigenvectors generated by the successor representation matrix are similar to those generated by the graph laplacian and show results on learning a deep successor representation on the Atari domain. 
I consider a maze for which initial exploration is unable to discover all states in the maze. The SR constructed therefore has value s of 0 for such states. Now, some of the eigenoption discovered for such a SR are able to reach the unexplored part of the state space, and thus relearn an SR for starting at the state where the corresponding option terminates. The relearnt SR can be used to generate eigenvectors again which now correspond to all of the space (which is now explored). This way eigenoptions can help in exploration. To achieve this, it was important for the eigenvector to have the states for the unexplored space as having value zero (as they remain unexplored) and the rest as having negative values (so as to motivate the agent to reach high reward states ie. unexplored space). However,  this cannot be assured in the function approximation setting, an therefore does not guarantee that we find / learn eigenoptions that can help in exploring. Learning eigenoption through Q learning is regarded as computationally expensive. This is because, we are required to learn almost 1024 options using Q learning on an Atari game, each of which takes more than at least a million steps. 
